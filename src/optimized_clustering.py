import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from sklearn.cluster import KMeans
from scipy.stats import entropy
from tqdm import tqdm
import joblib

try:
    import faiss
    HAS_FAISS = True
except ImportError:
    HAS_FAISS = False

class FaissKMeans:
    def __init__(self, n_clusters=8, n_init=10, max_iter=300, random_state=42, use_gpu=True):
        self.n_clusters = n_clusters
        self.n_init = n_init
        self.max_iter = max_iter
        self.random_state = random_state
        self.use_gpu = use_gpu
        self.cluster_centers_ = None
        self.inertia_ = None
        self.labels_ = None
        self.obj = None

    def fit(self, X):
        if not HAS_FAISS:
             raise ImportError("Faiss not installed")
        
        X = np.ascontiguousarray(X).astype('float32')
        d = X.shape[1]
        
        # Use simple faiss.Kmeans
        # Note: faiss.Kmeans uses 'seed' for random state if available in newer versions, 
        # but often it's global.
        kmeans = faiss.Kmeans(d, self.n_clusters, niter=self.max_iter, nredo=self.n_init, verbose=False, gpu=self.use_gpu, seed=self.random_state)
        kmeans.train(X)
        
        self.cluster_centers_ = kmeans.centroids
        self.inertia_ = kmeans.obj[-1] if hasattr(kmeans, 'obj') and len(kmeans.obj) > 0 else 0
        self.obj = kmeans
        
        # For labels, we need to search
        index = faiss.IndexFlatL2(d)
        if self.use_gpu:
            res = faiss.StandardGpuResources()
            index = faiss.index_cpu_to_gpu(res, 0, index)
            
        index.add(self.cluster_centers_)
        _, labels = index.search(X, 1)
        self.labels_ = labels.flatten()
        return self

    def fit_predict(self, X):
        self.fit(X)
        return self.labels_

    def predict(self, X):
        if self.cluster_centers_ is None:
            raise ValueError("Model not fitted yet")
            
        X = np.ascontiguousarray(X).astype('float32')
        d = X.shape[1]
        
        index = faiss.IndexFlatL2(d)
        if self.use_gpu and HAS_FAISS:
             res = faiss.StandardGpuResources()
             index = faiss.index_cpu_to_gpu(res, 0, index)

        index.add(self.cluster_centers_)
        _, labels = index.search(X, 1)
        return labels.flatten()

# ==========================================
# 1. è³‡æ–™é›†è®€å– (Updated for New Schema)
# ==========================================

class JudgmentDataset(Dataset):
    def __init__(self, parquet_paths, target_category=None):
        """
        è®€å– Parquet ä¸¦è§£æ Dense Vector èˆ‡ JTITLE
        Args:
            parquet_paths: å–®å€‹æª”æ¡ˆè·¯å¾‘æˆ–æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
            target_category: (Optional) æŒ‡å®šåªè™•ç† 'åˆ‘äº‹', 'æ°‘äº‹' æˆ– 'è¡Œæ”¿'
        """
        if isinstance(parquet_paths, str):
            parquet_paths = [parquet_paths]
            
        print(f"Loading datasets from {len(parquet_paths)} files...")
        
        # è®€å–ä¸¦åˆä½µæ‰€æœ‰ parquet
        df_list = []
        for p in parquet_paths:
            if os.path.exists(p):
                df_temp = pd.read_parquet(p)
                df_list.append(df_temp)
            else:
                print(f"Warning: File not found {p}")
        
        if not df_list:
            raise ValueError("No valid parquet files loaded.")
            
        self.df = pd.concat(df_list, ignore_index=True)
        
        # éæ¿¾ç‰¹å®šé¡åˆ¥ (å¦‚æœéœ€è¦)
        if target_category:
            print(f"Filtering for category: {target_category}")
            self.df = self.df[self.df['category'] == target_category].reset_index(drop=True)
            
        print(f"Total records loaded: {len(self.df)}")
        
        # [DEBUG] Removed sampling limit for production
        # if len(self.df) > 2000:
        #     print("âš ï¸ Sampling only 2000 records for quick testing...")
        #     self.df = self.df.sample(2000, random_state=42).reset_index(drop=True)
        
        print(f"Total records loaded: {len(self.df)}")
        
        # 1. è™•ç† Dense Vector
        # æª¢æŸ¥å‘é‡æ ¼å¼ï¼Œç¢ºä¿æ˜¯ numpy matrix (N, 1024)
        print("Processing dense vectors...")
        # å‡è¨­ parquet è®€å‡ºä¾†æ˜¯ numpy array æˆ– list
        first_vec = self.df['dense_vec'].iloc[0]
        if isinstance(first_vec, (list, np.ndarray)):
            # stack æœƒå°‡ list of arrays è½‰æˆ matrix
            self.vectors = np.stack(self.df['dense_vec'].values)
        else:
            raise ValueError("Format Error: 'dense_vec' column format is invalid.")

        # 2. è¨­å®šé—œéµæ¬„ä½
        # ä½¿ç”¨ jtitle ä½œç‚ºæ¡ˆç”±æ¨™ç±¤ (COA Label)
        self.df['coa_label'] = self.df['jtitle'].astype(str)
        
        # ä½¿ç”¨ disputability ä½œç‚ºä»£è¡¨æ€§æ¡æ¨£çš„ä¾æ“š (Target Mean)
        # ç¢ºä¿å®ƒæ˜¯æ•¸å€¼å‹åˆ¥
        self.df['disputability'] = pd.to_numeric(self.df['disputability'], errors='coerce').fillna(0)
        
        # 3. å»ºç«‹ COA ID æ˜ å°„
        self.coa_unique = self.df['coa_label'].unique()
        self.coa_to_id = {label: idx for idx, label in enumerate(self.coa_unique)}
        self.df['coa_id'] = self.df['coa_label'].map(self.coa_to_id)
        
        print(f"Dataset ready: {len(self.df)} cases, {len(self.coa_unique)} unique JTITLEs.")

    def get_coa_statistics(self):
        """
        è¨ˆç®—æ¯å€‹æ¡ˆç”±çš„å¹³å‡ Disputability (Methodology 4.2.2)
        é€™æ˜¯ Representative Sampling çš„åŸºæº–ï¼šæˆ‘å€‘è¦é¸å‡ºæœ€æ¥è¿‘é€™å€‹å¹³å‡å€¼çš„æ¡ˆä»¶
        """
        # éæ¿¾æ‰æ¨£æœ¬æ•¸éå°‘çš„æ¡ˆç”±ï¼Œé¿å…çµ±è¨ˆåå·® (é€™è£¡è¨­ç‚ºè‡³å°‘è¦æœ‰ 5 ç­†)
        counts = self.df['coa_label'].value_counts()
        valid_coas = counts[counts >= 5].index
        
        stats = self.df[self.df['coa_label'].isin(valid_coas)].groupby('coa_label')['disputability'].mean().to_dict()
        return stats

# ==========================================
# 2. æ ¸å¿ƒæ¼”ç®—æ³•ï¼šRepresentative Sampling & Dual-Objective Clustering
# ==========================================

class SemanticClusteringManager:
    def __init__(self, dataset, m_support=100):
        self.dataset = dataset
        self.m = m_support
        self.prototypes = []        # é¸å‡ºçš„ä»£è¡¨æ€§å‘é‡ (Sampled Vectors)
        self.prototype_coa_ids = [] # å°æ‡‰çš„æ¡ˆç”± ID
        
    def step1_representative_sampling(self):
        """
        Step 1: Representative Sampling
        é¸å‡ºæœ€æ¥è¿‘æ¯é«”å¹³å‡ Disputability çš„ m å€‹æ¡ˆä»¶ä½œç‚º Support Set
        """
        print("\n=== Step 1: Performing Representative Sampling ===")
        pop_means = self.dataset.get_coa_statistics()
        grouped = self.dataset.df.groupby('coa_label')
        
        selected_vectors = []
        selected_coa_ids = []
        skipped_coas = 0
        
        # éæ­·æ¯å€‹æ¡ˆç”± (JTITLE)
        for coa, group in tqdm(grouped, desc="Sampling COAs"):
            # å¦‚æœè©²æ¡ˆç”±ä¸åœ¨çµ±è¨ˆåå–®å…§ (æ¨£æœ¬å¤ªå°‘)ï¼Œç›´æ¥è·³éæˆ–å…¨å–
            if coa not in pop_means:
                # ç­–ç•¥ï¼šå¦‚æœæ¨£æœ¬æ¥µå°‘ (< 5)ï¼Œæˆ‘å€‘å¯ä»¥é¸æ“‡å¿½ç•¥ï¼Œæˆ–æ˜¯å…¨å–
                # é€™è£¡ç‚ºäº† Prototype å“è³ªï¼Œé¸æ“‡å¿½ç•¥æ¥µç¨€ç–æ¡ˆç”±
                skipped_coas += 1
                continue

            target_mean = pop_means[coa]
            candidates = group.index.tolist()
            
            # å¦‚æœè©²æ¡ˆç”±ç¸½æ•¸å°±å°‘æ–¼ m (ä¾‹å¦‚åªæœ‰ 8 ç­†)ï¼Œç›´æ¥å…¨éƒ¨ç´å…¥
            if len(group) <= self.m:
                chosen_indices = candidates
            else:
                # [æ¼”ç®—æ³•] éš¨æ©ŸæŠ½æ¨£ 50 æ¬¡ï¼Œé¸ mean æœ€æ¥è¿‘ target_mean çš„é‚£ä¸€çµ„
                best_sample = None
                min_diff = float('inf')
                
                # Monte Carlo Approximation for "argmin"
                for _ in range(50):
                    sample_indices = np.random.choice(candidates, self.m, replace=False)
                    sample_vals = self.dataset.df.loc[sample_indices, 'disputability'].values
                    sample_mean = np.mean(sample_vals)
                    diff = abs(sample_mean - target_mean)
                    
                    if diff < min_diff:
                        min_diff = diff
                        best_sample = sample_indices
                
                chosen_indices = best_sample
            
            # æ”¶é›†å‘é‡
            vecs = self.dataset.vectors[chosen_indices]
            selected_vectors.append(vecs)
            
            # è¨˜éŒ„é€™äº›å‘é‡å±¬æ–¼å“ªå€‹ COA ID
            coa_id = self.dataset.coa_to_id[coa]
            selected_coa_ids.extend([coa_id] * len(chosen_indices))
            
        self.prototypes = np.vstack(selected_vectors)
        self.prototype_coa_ids = np.array(selected_coa_ids)
        print(f"Sampling Complete.")
        print(f"Total Prototype Vectors: {len(self.prototypes)}")
        print(f"Skipped COAs (too few samples): {skipped_coas}")
        
    def step2_dual_objective_grid_search(self, k_min=10, k_max=100, step=5, lambda_div=0.1):
        """
        Step 2: Grid Search for Optimal K
        Minimize Total Loss = Cohesion Loss + lambda * Diversity Penalty
        """
        print("\n=== Step 2: Dual-Objective Grid Search for K ===")
        print(f"Search Range: {k_min} to {k_max}, Lambda: {lambda_div}")
        
        results = []
        best_k = -1
        best_loss = float('inf')
        best_model = None
        
        k_range = range(k_min, k_max + 1, step)
        
        for k in k_range:
            # 1. åŸ·è¡Œ K-Means
            kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
            labels = kmeans.fit_predict(self.prototypes)
            
            # 2. è¨ˆç®— Cohesion Loss (å¸Œæœ›åŒ JTITLE åœ¨ä¸€èµ·)
            cohesion_scores = []
            unique_coas = np.unique(self.prototype_coa_ids)
            
            for coa_id in unique_coas:
                mask = (self.prototype_coa_ids == coa_id)
                member_labels = labels[mask]
                
                if len(member_labels) > 0:
                    counts = np.bincount(member_labels)
                    n_max = counts.max()
                    n_total = len(member_labels) 
                    cohesion_scores.append(n_max / n_total)
            
            loss_cohesion = 1.0 - np.mean(cohesion_scores)
            
            # 3. è¨ˆç®— Diversity Penalty (å¸Œæœ› Cluster ä¸è¦å¤ªé›œ)
            entropy_scores = []
            total_samples = len(self.prototypes)
            
            for cluster_id in range(k):
                mask = (labels == cluster_id)
                member_coas = self.prototype_coa_ids[mask]
                
                if len(member_coas) > 0:
                    value_counts = pd.Series(member_coas).value_counts(normalize=True)
                    ent = entropy(value_counts.values)
                    weight = len(member_coas) / total_samples
                    entropy_scores.append(weight * ent)
            
            loss_diversity = np.sum(entropy_scores)
            
            # 4. ç¸½åˆ†
            total_loss = loss_cohesion + lambda_div * loss_diversity
            
            print(f"K={k:3d} | Total={total_loss:.4f} (Coh={loss_cohesion:.4f}, Div={loss_diversity:.4f})")
            
            results.append({
                'k': k, 'total': total_loss, 'coh': loss_cohesion, 'div': loss_diversity
            })
            
            if total_loss < best_loss:
                best_loss = total_loss
                best_k = k
                best_model = kmeans

        print(f"\nğŸ† Best K found: {best_k} (Loss: {best_loss:.4f})")
        return best_model, best_k, pd.DataFrame(results)

# ==========================================
# 3. åŸ·è¡Œæµç¨‹
# ==========================================

def main():
    # è¨­å®šåƒæ•¸
    # Use paths relative to this script
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    data_dir = os.path.join(project_root, "data", "datasets_parquet")

    # è‡ªå‹•å°‹æ‰¾è³‡æ–™é›†æª”æ¡ˆ
    parquet_files = []
    # é è¨­å°‹æ‰¾åˆ‘äº‹
    target_file = os.path.join(data_dir, "åˆ‘äº‹_continuous.parquet")
    if os.path.exists(target_file):
        parquet_files.append(target_file)
    else:
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå˜—è©¦åˆ—å‡ºç›®éŒ„ä¸‹æ‰€æœ‰ continuous.parquet
        if os.path.exists(data_dir):
            for f in os.listdir(data_dir):
                if f.endswith("_continuous.parquet"):
                    parquet_files.append(os.path.join(data_dir, f))
    
    if not parquet_files:
        print(f"Warning: No parquet files found in {data_dir}")

    # é€™è£¡æ¼”ç¤ºåªè·‘åˆ‘äº‹ (è‹¥æª”æ¡ˆåˆ—è¡¨æœ‰è®Šï¼Œéœ€ç›¸æ‡‰èª¿æ•´)
    target_category = "åˆ‘äº‹" 
    
    output_dir = "clustering_results"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. è¼‰å…¥è³‡æ–™ (è‡ªå‹•è®€å– jtitle èˆ‡ disputability)
    # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œè«‹ç¢ºä¿è·¯å¾‘æ­£ç¢º
    try:
        dataset = JudgmentDataset(parquet_files, target_category=target_category)
    except Exception as e:
        print(f"Error loading data: {e}")
        return

    # 2. åˆå§‹åŒ–åˆ†ç¾¤ç®¡ç†å™¨
    # m_support=100: æ¯å€‹æ¡ˆç”±æœ€å¤šå– 100 å€‹æ¨£æœ¬
    manager = SemanticClusteringManager(dataset, m_support=100)
    
    # 3. Step 1: Representative Sampling (ä½¿ç”¨ disputability ä½œç‚ºåŸºæº–)
    manager.step1_representative_sampling()
    
    # 4. Step 2: Grid Search K
    # æ ¹æ“šå–æ¨£å¾Œçš„æ¡ˆç”±æ•¸é‡å‹•æ…‹æ±ºå®šæœå°‹ç¯„åœ
    n_sampled_coas = len(np.unique(manager.prototype_coa_ids))
    print(f"Number of JTITLEs used for clustering: {n_sampled_coas}")
    
    if n_sampled_coas < 5:
        print("Not enough COAs to perform clustering.")
        return

    # æœå°‹ç¯„åœå»ºè­°ï¼šå¾ COA æ•¸é‡çš„ 5% åˆ° 60%
    k_min = max(2, int(n_sampled_coas * 0.05))
    k_max = min(n_sampled_coas, int(n_sampled_coas * 0.6))
    step = max(1, (k_max - k_min) // 10)
    
    best_kmeans, best_k, result_log = manager.step2_dual_objective_grid_search(
        k_min=k_min, k_max=k_max, step=step, lambda_div=0.1
    )
    
    # 5. æ‡‰ç”¨æœ€ä½³æ¨¡å‹ï¼šç‚ºæ‰€æœ‰è³‡æ–™åˆ†é… Cluster ID
    print("\nAssigning Cluster IDs to the entire dataset...")
    # ç”±æ–¼è³‡æ–™é‡å¯èƒ½å¾ˆå¤§ (10è¬ç­†+)ï¼Œåˆ†æ‰¹é æ¸¬ä»¥é˜² OOM (Optional but recommended)
    batch_size = 5000
    all_vectors = dataset.vectors
    all_cluster_ids = []
    
    for i in range(0, len(all_vectors), batch_size):
        batch = all_vectors[i : i + batch_size]
        ids = best_kmeans.predict(batch)
        all_cluster_ids.extend(ids)
        
    # å°‡çµæœå­˜å› DataFrame
    dataset.df['cluster_id'] = all_cluster_ids
    
    # 6. å„²å­˜çµæœ
    # (A) å„²å­˜è™•ç†å¥½çš„ Dataframe (å« cluster_id, jtitle, disputability)
    # å»ºè­°åŠ ä¸Š category å‰ç¶´ä»¥å…æ··æ·†
    output_name = f"{target_category}_clustered.parquet" if target_category else "all_clustered.parquet"
    output_parquet = os.path.join(output_dir, output_name)
    dataset.df.to_parquet(output_parquet)
    print(f"âœ… Saved clustered dataset to: {output_parquet}")
    
    # (B) å„²å­˜ KMeans æ¨¡å‹ (åŒ…å« Centroids)
    model_path = os.path.join(output_dir, f"kmeans_model_{target_category}.joblib")
    joblib.dump(best_kmeans, model_path)
    print(f"âœ… Saved KMeans model to: {model_path}")
    
    # (C) å„²å­˜ Search Log
    log_path = os.path.join(output_dir, "grid_search_log.csv")
    result_log.to_csv(log_path, index=False)
    
    print("\nPre-processing Complete!")

if __name__ == "__main__":
    main()